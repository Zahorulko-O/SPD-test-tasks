{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "### МЛ модель використовує фрейми з веб-камери на комп’ютері, розпізнає лице людини і обводить його прямокутником. Напиши скріпт який автоматично виведе це на екран. Виконай це завдання найзручнішим та найшвидшим шляхом. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порівняймо моделі багатозадачної каскадної згорткової нейронної мережі (MTCNN) та модуль DNN OpenCV.\n",
    "\n",
    "Модель Caffe базується на Single Shot-Multibox Detector (SSD) і використовує архітектуру ResNet-10 як свою основу (був представлений після OpenCV 3.3 у своєму модулі глибокої нейронної мережі).\n",
    "MTCNN був представлений Kaipeng Zhang та ін. у 2016 р. у своїй роботі « Спільне виявлення обличчя та вирівнювання за допомогою багатозадачних каскадних згорткових мереж ». У ньому використовується каскадна структура з трьома ступенями CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01AFE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01AFE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01AFE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01AFE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01AFE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "detector1 = MTCNN()\n",
    "modelFile = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4') #window with video from camera\n",
    "font = cv2.FONT_ITALIC #denotes the font type of text in video\n",
    "line = cv2.FILLED  #linetype\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        img = cv2.resize(img, None, fx=1, fy=1)\n",
    "        height, width = img.shape[:2]\n",
    "        img1 = img.copy()\n",
    "        img2 = img.copy()\n",
    "        img3 = img.copy()\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)),\n",
    "                                        1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces1 = detector1.detect_faces(img)# result for mtcnn\n",
    "        faces2 = net.forward() # result for dnn\n",
    "        \n",
    "    # display faces on the real time     \n",
    "        #MTCNN\n",
    "        for result in faces1:\n",
    "            x, y, w, h = result['box']\n",
    "            x1, y1 = x + w, y + h\n",
    "            #draw a rectangle (image, start_point, end_point, color, thickness):\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        #put text into video\n",
    "        #(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]]\n",
    "        cv2.putText(img, 'MTCNN', (30, 30), font, 1, (255, 255, 0), 2, line)\n",
    "\n",
    "        #dnn\n",
    "        for i in range(faces2.shape[2]):\n",
    "            confidence = faces2[0, 0, i, 2]\n",
    "            if confidence > 0.5: \n",
    "                box = faces2[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                (x, y, x1, y1) = box.astype(\"int\")\n",
    "                #draw a rectangle (image, start_point, end_point, color, thickness):\n",
    "                cv2.rectangle(img2, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        #put text into video \n",
    "        cv2.putText(img2, 'DNN', (30, 30), font, 1, (255, 255, 0), 2, line)\n",
    "        \n",
    "        h1 = cv2.hconcat([img3, img1])\n",
    "        h2 = cv2.hconcat([img, img2])\n",
    "        fin = cv2.vconcat([h1, h2])\n",
    "        \n",
    "        cv2.imshow(\"MTCNN\", img) #present the result\n",
    "        cv2.imshow(\"DNN\", img2) #present the result\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #press \"q\" to out\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "           \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подивимось на швидкість обробки відео:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "net = cv2.dnn.readNetFromCaffe(\"models/deploy.prototxt.txt\",\n",
    "                               \"models/res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4') #window with video from camera\n",
    "font = cv2.FONT_ITALIC #denotes the font type of text in video\n",
    "line = cv2.FILLED  #linetype\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        img = cv2.resize(img, None, fx=1, fy=1)\n",
    "        height, width = img.shape[:2]\n",
    "        img1 = img.copy()\n",
    "        img2 = img.copy()\n",
    "        img3 = img.copy()\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)),\n",
    "                                        1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces2 = net.forward() \n",
    "        \n",
    "         #put text into video\n",
    "                #dnn\n",
    "        for i in range(faces2.shape[2]):\n",
    "            confidence = faces2[0, 0, i, 2]\n",
    "            if confidence > 0.5: \n",
    "                box = faces2[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                (x, y, x1, y1) = box.astype(\"int\")\n",
    "                #draw a rectangle (image, start_point, end_point, color, thickness):\n",
    "                cv2.rectangle(img2, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        #put text into video \n",
    "        cv2.putText(img2,\n",
    "                    'DNN', (30, 30),\n",
    "                    font, 1, (255, 255, 0), 2, line)\n",
    "        \n",
    "        h1 = cv2.hconcat([img3, img1])\n",
    "        h2 = cv2.hconcat([img, img2])\n",
    "        fin = cv2.vconcat([h1, h2])\n",
    "        \n",
    "        cv2.imshow(\"DNN\", img2) #present the result\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #press \"q\" to out\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "           \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01651550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01651550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA01651550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "detector1 = MTCNN()\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4') #window with video from camera\n",
    "font = cv2.FONT_ITALIC #denotes the font type of text in video\n",
    "line = cv2.FILLED  #linetype\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        img = cv2.resize(img, None, fx=1, fy=1)\n",
    "        height, width = img.shape[:2]\n",
    "        img1 = img.copy()\n",
    "        img2 = img.copy()\n",
    "        img3 = img.copy()\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)),\n",
    "                                        1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces1 = detector1.detect_faces(img)# result for mtcnn\n",
    "        faces2 = net.forward() # result for dnn\n",
    "        \n",
    "    # display faces on the real time     \n",
    "        #MTCNN\n",
    "        for result in faces1:\n",
    "            x, y, w, h = result['box']\n",
    "            x1, y1 = x + w, y + h\n",
    "            #draw a rectangle (image, start_point, end_point, color, thickness):\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        #put text into video\n",
    "        \n",
    "        cv2.putText(img, 'MTCNN', (30, 30), font, 1, (255, 255, 0), 2, line)\n",
    "\n",
    "        h1 = cv2.hconcat([img3, img1])\n",
    "        h2 = cv2.hconcat([img, img2])\n",
    "        fin = cv2.vconcat([h1, h2])\n",
    "        \n",
    "        cv2.imshow(\"MTCNN\", img) #present the result\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #press \"q\" to out\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "           \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обробці завантаженого відео DNN знадобилось 13.9 s, а MTCNN - 1min 23s. Працює набагато швидше DNN.\n",
    "При тестуванні з камери  DNN  модель краще показала себе з оклюзією, швидкими рухами голови. Змогла ідентифікувати бічні грані. Гарно справилась з перепадами освітлення та перешкодами(частково закритим лицем). \n",
    "Висновок - використовуємо модель Caffe модуля DNN від OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\",\n",
    "                               \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "cap = cv2.VideoCapture(0) #window with video from camera\n",
    "font = cv2.FONT_ITALIC #denotes the font type of text in video\n",
    "line = cv2.FILLED  #linetype\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        img = cv2.resize(img, None, fx=1, fy=1)\n",
    "        height, width = img.shape[:2]\n",
    "        img1 = img.copy()\n",
    "        img2 = img.copy()\n",
    "        img3 = img.copy()\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)),\n",
    "                                        1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces2 = net.forward() \n",
    "        \n",
    "         #put text into video\n",
    "                #dnn\n",
    "        for i in range(faces2.shape[2]):\n",
    "            confidence = faces2[0, 0, i, 2]\n",
    "            if confidence > 0.5: \n",
    "                box = faces2[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                (x, y, x1, y1) = box.astype(\"int\")\n",
    "                #draw a rectangle (image, start_point, end_point, color, thickness):\n",
    "                cv2.rectangle(img2, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        #put text into video \n",
    "        cv2.putText(img2,\n",
    "                    'DNN', (30, 30),\n",
    "                    font, 1, (255, 255, 0), 2, line)\n",
    "        \n",
    "        h1 = cv2.hconcat([img3, img1])\n",
    "        h2 = cv2.hconcat([img, img2])\n",
    "        fin = cv2.vconcat([h1, h2])\n",
    "        \n",
    "        cv2.imshow(\"DNN\", img2) #present the result\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #press \"q\" to out\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "           \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
